{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "543ffd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as math\n",
    "import neighborhood as nbr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "import torch as torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b673729-3b3a-43e0-aaf7-00fd25e38478",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below is an import function written with some assistance from Josh Gorin to allow for an additional input into the neighborhood optimizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "813f38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(pd.DataFrame):\n",
    "    def __init__(self, data:pd.DataFrame):\n",
    "        super().__init__(data=data, index=data.index, columns=data.columns)\n",
    "        assert self[\"TC\"] is not None, \"given dataset does not contain TC parameter.\"\n",
    "        assert self[\"thr\"] is not None, \"given dataset does not contain thr parameter.\"\n",
    "        assert self[\"ln(D/a^2)\"] is not None, \"given dataset does not contain ln(D/a^2) parameter.\"\n",
    "        assert self[\"Fi\"] is not None, \"given dataset does not contain Fi parameter.\"\n",
    "\n",
    "        self.np_TC = torch.tensor(self[\"TC\"].values) \n",
    "        self.np_thr = torch.tensor(self[\"thr\"].values) \n",
    "        self.np_lnDaa = torch.tensor(self[\"ln(D/a^2)\"].values) \n",
    "        self.np_Fi_exp = torch.tensor(self[\"Fi\"].values) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af1245d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    def __init__(self, fn, data:Dataset):\n",
    "        self.fn = fn\n",
    "        self.dataset = data\n",
    "\n",
    "    def evaluate(self, X):\n",
    "        return self.fn(X, self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d81939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code written by Josh Gorin to allow for an additional input into the neighborhood optimizer function\n",
    "import neighborhood as nbr\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Optimizer(nbr.Searcher):\n",
    "    \n",
    "    def __init__(self, dataset_path:str, objective_fn, limits:list[float], num_samp:int, \n",
    "                 num_resamp:int, names:list[str], maximize:bool=False, verbose:bool=True):\n",
    "        \n",
    "        if names == None:\n",
    "            names = []\n",
    "            \n",
    "        super().__init__(\n",
    "            objective_fn, \n",
    "            limits, \n",
    "            num_samp, \n",
    "            num_resamp, \n",
    "            names=names, \n",
    "            maximize=maximize, \n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        self._objective = Objective(\n",
    "            objective_fn, \n",
    "            Dataset(pd.read_csv(dataset_path))\n",
    "        \n",
    "        )\n",
    "        \n",
    "    def update(self, num_iter=10):\n",
    "        \"\"\"\n",
    "        tweaked from original codebase to pass in training data into objective function\n",
    "        \"\"\"\n",
    "\n",
    "        for ii in range(num_iter):\n",
    "            \n",
    "            # If the first iteration, or every 10th... take a random sample\n",
    "            if not self._sample or ii % 10 == 0:\n",
    "                self._random_sample()\n",
    "            else:\n",
    "                self._neighborhood_sample()\n",
    "                        \n",
    "            # execute forward model for all samples in queue\n",
    "            while self._queue:\n",
    "                param = self._queue.pop()\n",
    "                result = self._objective.evaluate(param)\n",
    "                self._sample.append({\n",
    "                    'param': param,\n",
    "                    'result': result,\n",
    "                    'iter': self._iter\n",
    "                    })\n",
    "             \n",
    "            # prepare for next iteration\n",
    "            self._sample.sort(key=lambda x: x['result'], reverse=self._maximize)\n",
    "            self._iter += 1\n",
    "            if self._verbose:\n",
    "                print(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57649adf-6e29-43c9-85bc-ddcc75bd4f5b",
   "metadata": {},
   "source": [
    "The below will be a cell for calculating D0 and D/a^2 from experimental data. This code is adapted from Marissa Tremblay's 2014 implementation of the Fechtig and Kalbitzer equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8d762ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def D0calc_MonteCarloErrors(expdata):\n",
    "# Function for calculating D0 and D/a^2 from experimental data. Input should be a\n",
    "# Pandas DataFrame with columns \"TC\", \"thr\",\n",
    "# M, and, and delM, which correspond to heating temperature (deg C), \n",
    "# heating step duration (time in hours),\n",
    "# M (measured concentration in cps, atoms, or moles), delM (same units)\n",
    "    \n",
    "    # Calculate diffusivities from the previous experiment\n",
    "    TC = expdata.loc[:,\"TC\"].array\n",
    "    thr = expdata.loc[:,\"thr\"].array\n",
    "    M = expdata.loc[:,\"M\"].array\n",
    "    delM = expdata.loc[:,\"delM\"].array\n",
    "\n",
    "    #Check if units are in minutes and convert from hours to minutes if necesssary\n",
    "    if thr[1]>4:\n",
    "        thr = thr/60\n",
    "\n",
    "    #Convert units\n",
    "    TK = 273.15+TC\n",
    "    tsec = thr*60*60\n",
    "    Tplot = 1*10**4/TK\n",
    "    nstep = len(M)\n",
    "    cumtsec = np.cumsum(tsec)\n",
    "    Si = np.cumsum(M)\n",
    "    S = np.amax(Si)\n",
    "    Fi = Si/S\n",
    "\n",
    "\n",
    "    # initialize diffusivity vectors fore each Fechtig and Kalbitzer equation\n",
    "    DR2_a = np.zeros([nstep])\n",
    "    DR2_b = np.zeros([nstep])\n",
    "    DR2_c = np.zeros([nstep])\n",
    "\n",
    "    # Create the a list of times for each heating step\n",
    "    diffti = cumtsec[1:]-cumtsec[0:-1]\n",
    "\n",
    "    # Create a list of the gas fraction differences between steps\n",
    "    diffFi = Fi[1:]-Fi[0:-1]\n",
    "\n",
    "\n",
    "    # use equations 5a through c from Fechtig and Kalbitzer for spherical geometry\n",
    "    # Fechtig and Kalbitzer Equation 5a, for cumulative gas fractions up to 10%\n",
    "    # special case when i = 1; need to insert 0 for previous amount released\n",
    "\n",
    "    DR2_a[0] = ( (Fi[0]**2 - 0.**2 )*math.pi/(36*(cumtsec[0])))\n",
    "\n",
    "\n",
    "    # Equation 5a for all other steps\n",
    "\n",
    "    DR2_a[1:] = ((Fi[1:])**2 - (Fi[0:-1])**2 )*math.pi/(36*(diffti))\n",
    "\n",
    "    # Fechtig and Kalbitzer Equation 5b, for cumulative gas fractions between 10 and 90%\n",
    "\n",
    "    DR2_b[0] = (1/((math.pi**2)*tsec[0]))*((2*math.pi)-((math.pi*math.pi/3)*Fi[0])\\\n",
    "                                           - (2*math.pi)*(np.sqrt(1-(math.pi/3)*Fi[0])))\n",
    "    DR2_b[1:] = (1/((math.pi**2)*diffti))*(-(math.pi*math.pi/3)*diffFi \\\n",
    "                                           - (2*math.pi)*( np.sqrt(1-(math.pi/3)*Fi[1:]) \\\n",
    "                                            - np.sqrt(1 - (math.pi/3)*Fi[0:-1]) ))\n",
    "\n",
    "    # Fechtig and Kalbitzer Equation 5c, for cumulative gas fractions greater than 90%\n",
    "    DR2_c[1:] = (1/(math.pi*math.pi*diffti))*(np.log((1-Fi[0:-1])/(1-Fi[1:])))\n",
    "\n",
    "    # Decide which equation to use based on the cumulative gas fractions from each step\n",
    "    use_a = (Fi<= 0.1) & (Fi> 0.00000001)\n",
    "    use_b = (Fi > 0.1) & (Fi<= 0.85)\n",
    "    use_c = (Fi > 0.85) & (Fi<= 1.0)\n",
    "\n",
    "    # Compute the final values\n",
    "    DR2 = use_a*DR2_a + np.nan_to_num(use_b*DR2_b) + use_c*DR2_c\n",
    "\n",
    "    # Compute uncertainties in diffusivity using a Monte Carlo simulation\n",
    "    # Generates simulated step degassing datasets, such that each step of the \n",
    "    # experiment has a Gaussian distribution centered at M and with 1s.d. of\n",
    "    # delM across the simulated datasets.Then recomputes diffusivities for each \n",
    "    # simulated dataset and uses the range of diffusivities for each step across\n",
    "    # all simulated datasets to estimate uncertainty. \n",
    "    # make vector with correct diffusivites for each step\n",
    "\n",
    "    n_sim = 30000 #number of simulations in the monte carlo\n",
    "    MCsim = np.zeros([nstep,n_sim])#initialize matrix for simulated measurements\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(nstep):\n",
    "        #Generate the simulated measurements\n",
    "        MCsim[i,:] = np.random.randn(1,n_sim)*delM[i] + M[i]\n",
    "\n",
    "    #compute cumulative gas release fraction for each simulation\n",
    "    MCSi = np.cumsum(MCsim,0)\n",
    "    MCS = np.amax(MCSi,0)\n",
    "    MCFi = np.zeros([nstep,n_sim])\n",
    "    delMCFi = np.zeros([nstep,1])\n",
    "    MCFimean = np.zeros([nstep,1])\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(n_sim):\n",
    "        MCFi[:,i] = MCSi[:,i]/np.amax(MCSi[:,i])\n",
    "    for i in range(nstep):\n",
    "        delMCFi[i] = (np.amax(MCFi[i,:],0) - np.amin(MCFi[i,:],0))/2\n",
    "        MCFimean[i] = np.mean(MCFi[i,:],0)\n",
    "      \n",
    "    #Initialize vectors\n",
    "    MCDR2_a = np.zeros([nstep,n_sim])\n",
    "    MCDR2_b = np.zeros([nstep,n_sim])\n",
    "    MCDR2_c = np.zeros([nstep,n_sim])\n",
    "    MCdiffFi = np.zeros([nstep,n_sim])\n",
    "\n",
    "\n",
    "\n",
    "    for m in range(1,nstep): #For step of each experiment...\n",
    "        for n in range(n_sim):\n",
    "            MCdiffFi[m,n] = MCFi[m,n] - MCFi[m-1,n] #calculate the fraction released at each step\n",
    "    for n in range(n_sim): #For each first step of an experiment, insert 0 for previous amount released\n",
    "        MCDR2_a[0,n] = ((MCFi[m,n])**2 - (MCFi[m-1,n])**2 )*math.pi/(36*(diffti[m-1]))\n",
    "    for m in range(1,nstep): #Calculate fechtig and kalbitzer equations for each fraction\n",
    "        for n in range(n_sim):\n",
    "            MCDR2_a[m,n] = ( (MCFi[m,n])**2 - (MCFi[m-1,n])**2 )*math.pi/(36*(diffti[m-1]));\n",
    "            MCDR2_b[m,n] = (1/((math.pi**2)*diffti[m-1]))*( -(math.pi*math.pi/3)* MCdiffFi[m,n] \\\n",
    "                            - (2*math.pi)*( np.sqrt(1-(math.pi/3)*MCFi[m,n]) \\\n",
    "                            -np.sqrt(1 - (math.pi/3)*MCFi[m-1,n]) ))\n",
    "            MCDR2_c[m,n] = (1/(math.pi*math.pi*diffti[m-1]))*(np.log((1-MCFi[m-1,n])/(1-MCFi[m,n])));\n",
    "\n",
    "    use_a_MC = (MCFi<= 0.1) & (MCFi> 0.00000001)\n",
    "    use_b_MC = (MCFi > 0.1) & (MCFi<= 0.85)\n",
    "    use_c_MC = (MCFi > 0.85) & (MCFi<= 1.0) \n",
    "\n",
    "\n",
    "    MCDR2 = use_a_MC*MCDR2_a + np.nan_to_num(use_b_MC*MCDR2_b) + use_c_MC*MCDR2_c\n",
    "\n",
    "    MCDR2_uncert = np.zeros([nstep,1])\n",
    "    for i in range(nstep):\n",
    "        MCDR2_uncert[i,0] = np.std(MCDR2[i,:]) \n",
    "\n",
    "\n",
    "    return pd.DataFrame({\"Tplot\": Tplot,\"Fi\": MCFimean.ravel(),\"Fi uncertainty\": \\\n",
    "                               delMCFi.ravel(), \"Daa\": DR2,\"Daa uncertainty\": MCDR2_uncert.ravel(), \\\n",
    "                               \"ln(D/a^2)\": np.log(DR2),\"ln(D/a^2)-del\": np.log(DR2-MCDR2_uncert.ravel()), \\\n",
    "                               \"ln(D/a^2)+del\": np.log(DR2+MCDR2_uncert.ravel()) })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e40b68-31f3-4ec6-b04c-05e307776097",
   "metadata": {},
   "source": [
    "The following code is for use with the neighborhood algorithm as an objective function. It takes in diffusion kinetics (X) and experimental data (data) and returns a misfit between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaae3373-57a1-49b6-aea4-a307375b2486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forwardModelKinetics(kinetics,expData): \n",
    "    # kinetics: (Ea, lnd0aa_x, fracs_x). To make this compatible with other functions, if there are x fracs, input x-1 fractions, and the code will determine the\n",
    "    # final fraction.\n",
    "    \n",
    "    R = 0.008314 #gas constant\n",
    "    torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "     # Parameters that need to be read in (These I'll likely read from a file eventually\n",
    "    # But we'll read the data from above just to test the function for now...)\n",
    "    if len(kinetics) <= 3:\n",
    "        ndom = 1\n",
    "    else:\n",
    "        ndom = (len(kinetics))//2\n",
    "\n",
    "    # Make a subset of X, removing the Ea so that we have an even number of elements\n",
    "    temp = kinetics[1:]\n",
    " \n",
    "    if type(expData) is tuple:\n",
    "        TC = expData[0]\n",
    "        thr = expData[1]\n",
    "        lnDaa = expData[2]\n",
    "        Fi = expData[3]\n",
    "\n",
    "    else:\n",
    "        TC = expData.np_TC\n",
    "        thr = expData.np_thr\n",
    "        lnDaa = expData.np_lnDaa\n",
    "        Fi = expData.np_Fi_exp\n",
    "    \n",
    "    # Grab the parameters from the input\n",
    "    lnD0aa = torch.tile(temp[0:ndom],(len(thr),1)) #lnD0aa = np.tile(lnD0aa,(len(thr),1))\n",
    "    fracstemp = temp[ndom:]\n",
    "\n",
    "    fracs = torch.tile(torch.concat((fracstemp,1-torch.sum(fracstemp,axis=0,keepdim=True)),axis=-1),(len(thr),1))\n",
    "    Ea = torch.tile(kinetics[0],(len(thr),ndom)) # This is an Ea for each domain\n",
    "\n",
    "    # Put variables in correct units\n",
    "\n",
    "    tsec = torch.tile(torch.reshape(thr*3600,(-1,1)),(1,Ea.shape[1])) #This is a complicated way of getting tsec into a numdom x numstep matrix for multiplication\n",
    "    cumtsec = torch.tile(torch.reshape(torch.cumsum(thr*3600,dim=0),(-1,1)),(1,Ea.shape[1])) #Same as above, but for cumtsec                                                         \n",
    "    TK = torch.tile(torch.reshape((TC + 273.15),(-1,1)),(1,Ea.shape[1])) #This is a complicated way of turning TC from a 1-d array to a 2d array and making two column copies of it\n",
    "    \n",
    "\n",
    "    Daa = torch.exp(lnD0aa)*torch.exp(-Ea/(R*TK))\n",
    "    \n",
    "    # Pre-allocate fraction and Dtaa\n",
    "    f = torch.zeros(Daa.shape)\n",
    "    Dtaa = torch.zeros(Daa.shape)\n",
    "    DaaForSum = torch.zeros(Daa.shape)\n",
    "    \n",
    "    \n",
    "    #Calculate D\n",
    "    DaaForSum[0,:] = Daa[0,:]*tsec[0,:]\n",
    "    DaaForSum[1:,:] = Daa[1:,:]*(cumtsec[1:,:]-cumtsec[0:-1,:])\n",
    "\n",
    "    Dtaa = torch.cumsum(DaaForSum, axis = 0)\n",
    "\n",
    "    f = (6/(math.pi**(3/2)))*torch.sqrt((math.pi**2)*Dtaa)\n",
    "    f[f>=0.1] = (6/(torch.pi**(3/2)))*torch.sqrt((torch.pi**2)*Dtaa[f>=0.1])-(3/(torch.pi**2))* \\\n",
    "            ((torch.pi**2)*Dtaa[f>=0.1])\n",
    "    f[f>=0.9] = 1 - (6/(torch.pi**2))*torch.exp(-(torch.pi**2)*Dtaa[f>=0.9])\n",
    "\n",
    "    # If any member of f is preceeded by a value greater than it, set equal to 1 (we've reached total gas released)\n",
    "    f[1:,:][f[1:,:]<f[0:-1,:]] = 1\n",
    "    f[1:,:][f[0:-1,:]==1] = 1\n",
    "\n",
    "    # Multiply each gas realease by the percent gas located in each domain\n",
    "    f_MDD = f*fracs\n",
    "\n",
    "    # Calculate the total gas released at each step from each gas fraction\n",
    "    sumf_MDD = torch.sum(f_MDD,axis=1)\n",
    "    \n",
    "\n",
    "    #Calculate the apparent Daa from the MDD using equations of Fechtig and Kalbitzer\n",
    "    Daa_MDD_a = torch.zeros(sumf_MDD.shape)\n",
    "    Daa_MDD_b = torch.zeros(sumf_MDD.shape)\n",
    "    Daa_MDD_c = torch.zeros(sumf_MDD.shape)\n",
    "   \n",
    "    # use equations 5a through c from Fechtig and Kalbitzer for spherical geometry\n",
    "    # Fechtig and Kalbitzer Equation 5a, for cumulative gas fractions up to 10%\n",
    "    # special case when i = 1; need to insert 0 for previous amount released\n",
    "\n",
    "    # Rewrite the cumtsec as just one column to make for easier calculations below \n",
    "    # and because we don't need to use on three domains separately anymore\n",
    "    cumtsec = cumtsec[:,0]\n",
    "    diffti = cumtsec[1:]-cumtsec[0:-1]\n",
    "    diffFi = sumf_MDD[1:]-sumf_MDD[0:-1]\n",
    "\n",
    "    Daa_MDD_a[0] = ( (sumf_MDD[0]**2 - 0.**2 )*torch.pi/(36*(cumtsec[0])))\n",
    "\n",
    "\n",
    "    # Equation 5a for all other steps\n",
    "\n",
    "    Daa_MDD_a[1:] = ((sumf_MDD[1:])**2 - (sumf_MDD[0:-1])**2 )*math.pi/(36*(diffti))\n",
    "\n",
    "    # Fechtig and Kalbitzer Equation 5b, for cumulative gas fractions between 10 and 90%\n",
    "    Daa_MDD_b[0] = (1/((torch.pi**2)*tsec[0,0]))*((2*torch.pi)-((math.pi*math.pi/3)*sumf_MDD[0])\\\n",
    "                                           - (2*math.pi)*(torch.sqrt(1-(math.pi/3)*sumf_MDD[0])))\n",
    "    Daa_MDD_b[1:] = (1/((math.pi**2)*diffti))*(-(math.pi*math.pi/3)*diffFi \\\n",
    "                                           - (2*math.pi)*( torch.sqrt(1-(math.pi/3)*sumf_MDD[1:]) \\\n",
    "                                            - torch.sqrt(1 - (math.pi/3)*sumf_MDD[0:-1]) ))\n",
    "\n",
    "    # Fechtig and Kalbitzer Equation 5c, for cumulative gas fractions greater than 90%\n",
    "    Daa_MDD_c[1:] = (1/(math.pi*math.pi*diffti))*(torch.log((1-sumf_MDD[0:-1])/(1-sumf_MDD[1:])))\n",
    "\n",
    "    # Decide which equation to use based on the cumulative gas fractions from each step\n",
    "    use_a = (sumf_MDD<= 0.1) & (sumf_MDD> 0.00000001)\n",
    "    use_b = (sumf_MDD > 0.1) & (sumf_MDD<= 0.85)\n",
    "    use_c = (sumf_MDD > 0.85) & (sumf_MDD<= 1.0)\n",
    "    Daa_MDD = use_a*Daa_MDD_a + torch.nan_to_num(use_b*Daa_MDD_b) + use_c*Daa_MDD_c\n",
    "    \n",
    "\n",
    "    lnDaa_MDD = torch.log(Daa_MDD)\n",
    "    \n",
    "    return (TC,lnDaa,sumf_MDD)\n",
    "    #return pd.DataFrame({\"TC\": TC.ravel(),\"ln(D/a^2)_MDD\": lnDaa_MDD.ravel(),\"Fi_MDD\": sumf_MDD.ravel()}) # Return a dataframe with the results forward modeled   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3de16a5e-8aec-4720-a623-d501ff76d4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objectiveFunction(X,data):\n",
    "    torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "    # Below here will eventually get turned into a function\n",
    "    # Code written by Marissa Tremblay and modified/transcribed into Python by Drew Gorin.\n",
    "    #Last modified 1.2023.\n",
    "\n",
    "    # This function calculates the fraction of gas released from each domain\n",
    "    # in an MDD model during the heating schedule used in the diffusion\n",
    "    # experiment. Then the fractions released from each domain are combined in\n",
    "    # proportion to one another as specified by the MDD model, and the\n",
    "    # diffusivity of each step is calculated. A residual is calculated as the\n",
    "    # sum of absolute differences between the observed and modeled release\n",
    "    # fractions over all steps.\n",
    "    \n",
    "    #Time both of these\n",
    "    #X = torch.from_numpy(X)\n",
    "    X = torch.as_tensor(X)\n",
    "    # Unpack the parameters and spit out a high misfit value if constraints are violated\n",
    "    if len(X) <= 3:\n",
    "        ndom = 1\n",
    "    else:\n",
    "        ndom = (len(X))//2\n",
    "\n",
    "\n",
    "    # Grab the other parameters from the input\n",
    "    \n",
    "    temp = X[1:]\n",
    "    lnD0aa = temp[0:ndom]\n",
    "    fracstemp = temp[ndom:] \n",
    "    sumTemp = (1-torch.sum(fracstemp,axis=0,keepdim = True))\n",
    "    fracs = torch.concat((fracstemp,sumTemp),dim=-1)\n",
    "    # Report high misfit values if conditions are not met\n",
    "\n",
    "    \n",
    "    for i in range(len(lnD0aa)-1):\n",
    "        if lnD0aa[i+1]>lnD0aa[i]:\n",
    "            return 10**10\n",
    "        \n",
    "    fwdModelResults = forwardModelKinetics(X,data)\n",
    "\n",
    "    # Parameters that need to be read in (These I'll likely read from a file eventually\n",
    "    # But we'll read the data from above just to test the function for now...)\n",
    "\n",
    "    TC = data.np_TC #data[\"TC\"].to_numpy()\n",
    "    thr = data.np_thr#[\"thr\"].to_numpy()\n",
    "    lnDaa = data.np_lnDaa#[\"ln(D/a^2)\"].to_numpy()\n",
    "    Fi_exp = data.np_Fi_exp#[\"Fi\"].to_numpy()\n",
    "    Fi_MDD = fwdModelResults[2]#fwdModelResults[\"Fi_MDD\"].to_numpy()\n",
    "\n",
    "    \n",
    "    #Calculate the Fraction released for each heating step\n",
    "    TrueFracFi = (Fi_exp[1:]-Fi_exp[0:-1])\n",
    "    TrueFracFi = torch.concat((torch.unsqueeze(Fi_exp[0],dim=-1),TrueFracFi),dim=-1)\n",
    "    \n",
    "\n",
    "    trueFracMDD = Fi_MDD[1:]-Fi_MDD[0:-1]\n",
    "    trueFracMDD = torch.concat((torch.unsqueeze(Fi_MDD[0],dim=-1),trueFracMDD),dim=-1)\n",
    "    # Calculate the L1 loss \n",
    "    misfit = torch.absolute(TrueFracFi-trueFracMDD)\n",
    "\n",
    "    # Assign penalty for each step if the model runs out of gas before the experiment did\n",
    "\n",
    "    # Add a misfit penalty of 1 for each heating step that ran out of gas early\n",
    "    misfit[trueFracMDD==0] = 1\n",
    "    \n",
    "    if torch.round(Fi_MDD[-1],decimals=2) != 1:\n",
    "         return 10**10\n",
    "    \n",
    "\n",
    "#     if torch.round(Fi_MDD[-2],decimals=2) >= 1:\n",
    "#          return 10**10\n",
    "   # Return the sum of the residuals\n",
    "    return torch.sum(misfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a1a1c74-c18f-4fc7-8d77-c0a6ee01114e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I wrote this to accomodate a python optimizer at one point, but this is now out of date. don't use!\n",
    "\n",
    "def objectiveFunction4PythonOptimizer(X,*data):\n",
    "\n",
    "    # *data is a tuple of the data from the experiment in the order (\"TC\", \"thr\", \"ln(D/a^2)\", \"Fi\")\n",
    "    \n",
    "     # Below here will eventually get turned into a function\n",
    "    # Code written by Marissa Tremblay and modified/transcribed into Python by Drew Gorin.\n",
    "    #Last modified 1.2023.\n",
    "\n",
    "    # This function calculates the fraction of gas released from each domain\n",
    "    # in an MDD model during the heating schedule used in the diffusion\n",
    "    # experiment. Then the fractions released from each domain are combined in\n",
    "    # proportion to one another as specified by the MDD model, and the\n",
    "    # diffusivity of each step is calculated. A residual is calculated as the\n",
    "    # sum of absolute differences between the observed and modeled release\n",
    "    # fractions over all steps.\n",
    "\n",
    "    \n",
    "    # Unpack the parameters and spit out a high misfit value if constraints are violated\n",
    "    if len(X) <= 3:\n",
    "        ndom = 1\n",
    "    else:\n",
    "        ndom = (len(X))//2\n",
    "\n",
    "\n",
    "    # Grab the other parameters from the input\n",
    "    \n",
    "    temp = X[1:]\n",
    "    lnD0aa = temp[0:ndom]\n",
    "    fracstemp = temp[ndom:]\n",
    "    if torch.sum(fracstemp) >=1:\n",
    "        return 10**10\n",
    "    \n",
    "    fracs = torch.append(fracstemp,1-torch.sum(fracstemp))\n",
    "    \n",
    "    # Report high misfit values if conditions are not met\n",
    "    if torch.sum(fracs)!=1:\n",
    "        return 10**10\n",
    "    \n",
    "    for i in range(len(lnD0aa)-1):\n",
    "        if lnD0aa[i+1]>lnD0aa[i]:\n",
    "            return 10**10\n",
    "        \n",
    "        \n",
    "    fwdModelResults = forwardModelKinetics(X,data)\n",
    "\n",
    "    # Parameters that need to be read in (These I'll likely read from a file eventually\n",
    "    # But we'll read the data from above just to test the function for now...)\n",
    "    \n",
    "    TC = data[0]\n",
    "    thr = data[1]\n",
    "    lnDaa = data[2]\n",
    "    Fi_exp = data[3]\n",
    "    Fi_MDD = fwdModelResults[\"Fi_MDD\"].to_numpy()\n",
    "\n",
    "    #Calculate the Fraction released for each heating step\n",
    "    trueFracFi = (Fi_exp[1:]-Fi_exp[0:-1])\n",
    "    trueFracFi = torch.insert(trueFracFi,0,Fi_exp[0])\n",
    "\n",
    "    trueFracMDD = Fi_MDD[1:]-Fi_MDD[0:-1]\n",
    "    trueFracMDD = torch.insert(trueFracMDD,0,Fi_MDD[0])\n",
    "\n",
    "    # Calculate the L1 loss \n",
    "    misfit = torch.absolute(trueFracFi-trueFracMDD)\n",
    "\n",
    "    # Assign penalty for each step if the model runs out of gas before the experiment did\n",
    "\n",
    "    # Add a misfit penalty of 1 for each heating step that ran out of gas early\n",
    "    misfit[trueFracMDD==0] = 1\n",
    "\n",
    "    # Return the sum of the residuals\n",
    "    if Fi_MDD[-1] != 1:\n",
    "        return 10**10\n",
    "        \n",
    "    return torch.sum(misfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f2cfdee-c64f-4e8c-99a3-04e8bb8392eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModelResults(fwdModel,expData):\n",
    "    # Calculate the temp in 10000/TK (Standard plotting units for this field)\n",
    "    expData[\"10000/TK\"] = 10000/(expData[\"TC\"]+273.15)\n",
    "    fwdModel[\"10000/TK\"] = 10000/(fwdModel[\"TC\"]+273.15)\n",
    "    fwdModel[\"MDDFi_Cum\"] = np.cumsum(fwdModel[\"Fi_MDD\"])\n",
    "    expData[\"Fi_Cum\"] = np.cumsum(expData[\"Fi\"])\n",
    "    \n",
    "    \n",
    "    plt.figure();\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(data = expData, x = \"10000/TK\", y = \"ln(D/a^2)\")\n",
    "    sns.scatterplot(data = fwdModel, x = \"10000/TK\",y=\"ln(D/a^2)_MDD\")\n",
    "    plt.legend([\"Experiment\",\"Model\"])\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(data = expData, x =expData.index, y = \"Fi\")\n",
    "    sns.scatterplot(data = fwdModel, x =fwdModel.index, y=\"Fi_MDD\")\n",
    "    plt.legend([\"Experiment\",\"Model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e90594f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the MAIN function\n",
    "\n",
    "#Code written by Marissa Tremblay and transcribed into Python/modified by Drew Gorin. Last modified 1.2023\n",
    "\n",
    "#This m-file is used to fit an MDD model to stepwise degassing diffusion\n",
    "#experiment data. It is currently set up for only one isotope. The number\n",
    "#of domains is allowed to vary. The activation energy is assumed to be the\n",
    "#same across domains while the pre-exponential factor (D0/a^2) and the\n",
    "#fraction of gas in each domain varies. Needs the companion functions\n",
    "#D0calc_MonteCarloErros.m and TremblayMDD.m.\n",
    "\n",
    "# USER INPUT HERE (should be a csv with no header)\n",
    "nameOfInputCSVFile = \"3Domains.csv\"\n",
    "nameOfExperimentalResultsFile = \"data4Optimizer.csv\"\n",
    "\n",
    "## USER SHOULD NOT MODIFY LINES BELOW THIS\n",
    "\n",
    "\n",
    "expData = pd.read_csv(nameOfInputCSVFile,header=None)\n",
    "    \n",
    "#If extra columns get read in, trim them down to just 3\n",
    "if expData.shape[1] >=4:\n",
    "    expData = expData.loc[:,1:4]\n",
    "    \n",
    "# Name the columsn of the iput data\n",
    "expData.columns = [\"TC\", \"thr\",\"M\", \"delM\"]\n",
    "\n",
    "\n",
    "# Calculate Daa from experimental results\n",
    "expResults = D0calc_MonteCarloErrors(expData)\n",
    "\n",
    "# Combine the diffusion parameters with the experimental setup (T, thr, M, delM)\n",
    "# to get a final dataframe that will be passed into the optimizer\n",
    "diffusionExperimentResults = expData.join(expResults)\n",
    "\n",
    "# Write dataframe to a .csv file\n",
    "diffusionExperimentResults.to_csv(nameOfExperimentalResultsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b33d818-3719-4266-acfa-b437a4e29522",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\3965457553.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m110\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiffusionExperimentResults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TC\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiffusionExperimentResults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"thr\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiffusionExperimentResults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ln(D/a^2)\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiffusionExperimentResults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Fi\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual_annealing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjectiveFunction4PythonOptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_dual_annealing.py\u001b[0m in \u001b[0;36mdual_annealing\u001b[1;34m(func, bounds, args, maxiter, minimizer_kwargs, initial_temp, restart_temp_ratio, visit, accept, maxfun, seed, no_local_search, callback, x0, local_search_options)\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[1;31m# Initialization of the energy state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[0menergy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnergyState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m     \u001b[0menergy_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_wrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m     \u001b[1;31m# Minimum value of annealing temperature reached to perform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[1;31m# re-annealing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_dual_annealing.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, func_wrapper, rand_gen, x0)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mreinit_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0minit_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_energy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_wrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_energy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Objective function is returning None'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_dual_annealing.py\u001b[0m in \u001b[0;36mfun\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\4002644096.py\u001b[0m in \u001b[0;36mobjectiveFunction4PythonOptimizer\u001b[1;34m(X, *data)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mlnD0aa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mndom\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mfracstemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mndom\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfracstemp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sum(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# # Now I'll try the same optimization with the built in scipy global optimizer to see if it does a better job or is faster...\n",
    "# from scipy import optimize\n",
    "# bounds = [(70,110),(0,25),(0,25),(0,25),(10**(-10),1),(10**(-10),1)]\n",
    "# args = (diffusionExperimentResults[\"TC\"].to_numpy(),diffusionExperimentResults[\"thr\"].to_numpy(), diffusionExperimentResults[\"ln(D/a^2)\"].to_numpy(),diffusionExperimentResults[\"Fi\"].to_numpy())\n",
    "# results = optimize.dual_annealing(objectiveFunction4PythonOptimizer,bounds,args = args)\n",
    "\n",
    "\n",
    "# fwdModel = forwardModelKinetics(results.x,diffusionExperimentResults)\n",
    "# plotModelResults(fwdModel,diffusionExperimentResults)\n",
    "\n",
    "# diffusionExperimentResults\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a4fd8",
   "metadata": {},
   "source": [
    "# Some Notes for Josh \n",
    "\n",
    "The inputs to the optimizer have some constraints, and you can see that I've badly enforced them by assigning high misfit values if they aren't satisfied. \n",
    "\n",
    "## Model has $2* num Domains +1$ parameters. They are as follows:\n",
    "\n",
    "#### Ea: Activation energy in kj/mol\n",
    "\n",
    "#### $\\ln(D_0/a^2)$: in (1/s). There is one of these per domain\n",
    "\n",
    "#### Fraction of total gas (There are $numDom-1$ of these because $\\sum_{i=1}^{\\#Dom} Frac_i = 1$, so if you know $numDom-1$ of them, you know the value of last Frac).\n",
    "\n",
    "## The input parameters vector is as follows: [(Ea, lnd0aa1, lnd0aa2,...lnd0aa_x,Frac_1,Frac_2,...Frac_(numDom-1))]\n",
    "\n",
    "\n",
    "### The constraints on the inputs are as follows:\n",
    "\n",
    "#### $\\ln(D_0/a^2)_1 > ln(D_0/a^2)_2 > ... ln(D_0/a^2)_{numDom}  $\n",
    "\n",
    "#### $Fracs_1 + Fracs_2 + Fracs3... + Fracs_ndom = 1  $ However, we only stick the first numDom-1 into the optimizer, since the last Frac is determined by the other numDom-1 params\n",
    "\n",
    "#### Fi_MDD[-1] must = 1\n",
    "\n",
    "\n",
    "Note: The data we're playing with is a synthetic dataset I created with known values for all of these parameters and it has 3 domains. When solving for real data, we won't know the number of domains, but this is a good way for us to make sure the optimizer is behaving properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fdeb6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samp = 3\n",
      "num_resamp = 3\n",
      "Optimizer(iteration=1, samples=8, best=1.000000e+10)\n",
      "Optimizer(iteration=2, samples=16, best=2.348097e+01)\n",
      "Optimizer(iteration=3, samples=24, best=2.348097e+01)\n",
      "Optimizer(iteration=4, samples=32, best=2.348097e+01)\n",
      "Optimizer(iteration=5, samples=40, best=2.348097e+01)\n",
      "Optimizer(iteration=6, samples=48, best=2.348097e+01)\n",
      "Optimizer(iteration=7, samples=56, best=1.306059e+00)\n",
      "Optimizer(iteration=8, samples=64, best=1.306059e+00)\n",
      "Optimizer(iteration=9, samples=72, best=1.306059e+00)\n",
      "Optimizer(iteration=10, samples=80, best=1.306059e+00)\n",
      "Optimizer(iteration=11, samples=88, best=1.306059e+00)\n",
      "Optimizer(iteration=12, samples=96, best=5.280250e-01)\n",
      "Optimizer(iteration=13, samples=104, best=5.280250e-01)\n",
      "Optimizer(iteration=14, samples=112, best=5.280250e-01)\n",
      "Optimizer(iteration=15, samples=120, best=5.280250e-01)\n",
      "Optimizer(iteration=16, samples=128, best=5.280250e-01)\n",
      "Optimizer(iteration=17, samples=136, best=5.280250e-01)\n",
      "Optimizer(iteration=18, samples=144, best=5.280250e-01)\n",
      "Optimizer(iteration=19, samples=152, best=5.280250e-01)\n",
      "Optimizer(iteration=20, samples=160, best=5.280250e-01)\n",
      "Optimizer(iteration=21, samples=168, best=5.280250e-01)\n",
      "Optimizer(iteration=22, samples=176, best=5.280250e-01)\n",
      "Optimizer(iteration=23, samples=184, best=5.280250e-01)\n",
      "Optimizer(iteration=24, samples=192, best=5.280250e-01)\n",
      "Optimizer(iteration=25, samples=200, best=5.280250e-01)\n",
      "Optimizer(iteration=26, samples=208, best=5.280250e-01)\n",
      "Optimizer(iteration=27, samples=216, best=2.577218e-01)\n",
      "Optimizer(iteration=28, samples=224, best=2.577218e-01)\n",
      "Optimizer(iteration=29, samples=232, best=2.577218e-01)\n",
      "Optimizer(iteration=30, samples=240, best=2.577218e-01)\n",
      "Optimizer(iteration=31, samples=248, best=2.577218e-01)\n",
      "Optimizer(iteration=32, samples=256, best=2.577218e-01)\n",
      "Optimizer(iteration=33, samples=264, best=2.577218e-01)\n",
      "Optimizer(iteration=34, samples=272, best=2.577218e-01)\n",
      "Optimizer(iteration=35, samples=280, best=2.577218e-01)\n",
      "Optimizer(iteration=36, samples=288, best=1.976051e-01)\n",
      "Optimizer(iteration=37, samples=296, best=1.976051e-01)\n",
      "Optimizer(iteration=38, samples=304, best=1.976051e-01)\n",
      "Optimizer(iteration=39, samples=312, best=1.976051e-01)\n",
      "Optimizer(iteration=40, samples=320, best=1.976051e-01)\n",
      "Optimizer(iteration=41, samples=328, best=1.976051e-01)\n",
      "Optimizer(iteration=42, samples=336, best=1.976051e-01)\n",
      "Optimizer(iteration=43, samples=344, best=1.976051e-01)\n",
      "Optimizer(iteration=44, samples=352, best=1.976051e-01)\n",
      "Optimizer(iteration=45, samples=360, best=1.976051e-01)\n",
      "Optimizer(iteration=46, samples=368, best=1.915967e-01)\n",
      "Optimizer(iteration=47, samples=376, best=1.915967e-01)\n",
      "Optimizer(iteration=48, samples=384, best=1.667574e-01)\n",
      "Optimizer(iteration=49, samples=392, best=1.667574e-01)\n",
      "Optimizer(iteration=50, samples=400, best=1.667574e-01)\n",
      "Optimizer(iteration=51, samples=408, best=1.667574e-01)\n",
      "Optimizer(iteration=52, samples=416, best=1.590072e-01)\n",
      "Optimizer(iteration=53, samples=424, best=1.590072e-01)\n",
      "Optimizer(iteration=54, samples=432, best=1.590072e-01)\n",
      "Optimizer(iteration=55, samples=440, best=1.590072e-01)\n",
      "Optimizer(iteration=56, samples=448, best=1.590072e-01)\n",
      "Optimizer(iteration=57, samples=456, best=1.590072e-01)\n",
      "Optimizer(iteration=58, samples=464, best=1.590072e-01)\n",
      "Optimizer(iteration=59, samples=472, best=1.590072e-01)\n",
      "Optimizer(iteration=60, samples=480, best=1.590072e-01)\n",
      "Optimizer(iteration=61, samples=488, best=1.590072e-01)\n",
      "Optimizer(iteration=62, samples=496, best=1.590072e-01)\n",
      "Optimizer(iteration=63, samples=504, best=1.590072e-01)\n",
      "Optimizer(iteration=64, samples=512, best=1.546823e-01)\n",
      "Optimizer(iteration=65, samples=520, best=1.546823e-01)\n",
      "Optimizer(iteration=66, samples=528, best=1.417338e-01)\n",
      "Optimizer(iteration=67, samples=536, best=1.417338e-01)\n",
      "Optimizer(iteration=68, samples=544, best=1.417338e-01)\n",
      "Optimizer(iteration=69, samples=552, best=1.417338e-01)\n",
      "Optimizer(iteration=70, samples=560, best=1.417338e-01)\n",
      "Optimizer(iteration=71, samples=568, best=1.417338e-01)\n",
      "Optimizer(iteration=72, samples=576, best=1.417338e-01)\n",
      "Optimizer(iteration=73, samples=584, best=1.417338e-01)\n",
      "Optimizer(iteration=74, samples=592, best=1.401669e-01)\n",
      "Optimizer(iteration=75, samples=600, best=1.226297e-01)\n",
      "Optimizer(iteration=76, samples=608, best=1.226297e-01)\n",
      "Optimizer(iteration=77, samples=616, best=1.226297e-01)\n",
      "Optimizer(iteration=78, samples=624, best=1.226297e-01)\n",
      "Optimizer(iteration=79, samples=632, best=1.226297e-01)\n",
      "Optimizer(iteration=80, samples=640, best=1.226297e-01)\n",
      "Optimizer(iteration=81, samples=648, best=1.226297e-01)\n",
      "Optimizer(iteration=82, samples=656, best=1.226297e-01)\n",
      "Optimizer(iteration=83, samples=664, best=1.226297e-01)\n",
      "Optimizer(iteration=84, samples=672, best=1.226297e-01)\n",
      "Optimizer(iteration=85, samples=680, best=1.129314e-01)\n",
      "Optimizer(iteration=86, samples=688, best=1.129314e-01)\n",
      "Optimizer(iteration=87, samples=696, best=1.101684e-01)\n",
      "Optimizer(iteration=88, samples=704, best=1.101684e-01)\n",
      "Optimizer(iteration=89, samples=712, best=1.101684e-01)\n",
      "Optimizer(iteration=90, samples=720, best=1.101684e-01)\n",
      "Optimizer(iteration=91, samples=728, best=1.101684e-01)\n",
      "Optimizer(iteration=92, samples=736, best=1.101684e-01)\n",
      "Optimizer(iteration=93, samples=744, best=9.989178e-02)\n",
      "Optimizer(iteration=94, samples=752, best=9.989178e-02)\n",
      "Optimizer(iteration=95, samples=760, best=9.989178e-02)\n",
      "Optimizer(iteration=96, samples=768, best=9.989178e-02)\n",
      "Optimizer(iteration=97, samples=776, best=9.662066e-02)\n",
      "Optimizer(iteration=98, samples=784, best=9.662066e-02)\n",
      "Optimizer(iteration=99, samples=792, best=8.889952e-02)\n",
      "Optimizer(iteration=100, samples=800, best=8.889952e-02)\n",
      "Optimizer(iteration=101, samples=808, best=8.889952e-02)\n",
      "Optimizer(iteration=102, samples=816, best=8.889952e-02)\n",
      "Optimizer(iteration=103, samples=824, best=8.889952e-02)\n",
      "Optimizer(iteration=104, samples=832, best=8.889952e-02)\n",
      "Optimizer(iteration=105, samples=840, best=8.889952e-02)\n",
      "Optimizer(iteration=106, samples=848, best=8.889952e-02)\n",
      "Optimizer(iteration=107, samples=856, best=8.889952e-02)\n",
      "Optimizer(iteration=108, samples=864, best=8.889952e-02)\n",
      "Optimizer(iteration=109, samples=872, best=8.889952e-02)\n",
      "Optimizer(iteration=110, samples=880, best=8.889952e-02)\n",
      "Optimizer(iteration=111, samples=888, best=8.889952e-02)\n",
      "Optimizer(iteration=112, samples=896, best=8.889952e-02)\n",
      "Optimizer(iteration=113, samples=904, best=8.889952e-02)\n",
      "Optimizer(iteration=114, samples=912, best=8.889952e-02)\n",
      "Optimizer(iteration=115, samples=920, best=8.599903e-02)\n",
      "Optimizer(iteration=116, samples=928, best=8.599903e-02)\n",
      "Optimizer(iteration=117, samples=936, best=8.510419e-02)\n",
      "Optimizer(iteration=118, samples=944, best=8.510419e-02)\n",
      "Optimizer(iteration=119, samples=952, best=8.510419e-02)\n",
      "Optimizer(iteration=120, samples=960, best=8.426342e-02)\n",
      "Optimizer(iteration=121, samples=968, best=8.426342e-02)\n",
      "Optimizer(iteration=122, samples=976, best=8.426342e-02)\n",
      "Optimizer(iteration=123, samples=984, best=8.426342e-02)\n",
      "Optimizer(iteration=124, samples=992, best=8.426342e-02)\n",
      "Optimizer(iteration=125, samples=1000, best=8.334433e-02)\n",
      "Optimizer(iteration=126, samples=1008, best=7.791750e-02)\n",
      "Optimizer(iteration=127, samples=1016, best=7.791750e-02)\n",
      "Optimizer(iteration=128, samples=1024, best=7.791750e-02)\n",
      "Optimizer(iteration=129, samples=1032, best=7.791750e-02)\n",
      "Optimizer(iteration=130, samples=1040, best=7.791750e-02)\n",
      "Optimizer(iteration=131, samples=1048, best=7.791750e-02)\n",
      "Optimizer(iteration=132, samples=1056, best=7.791750e-02)\n",
      "Optimizer(iteration=133, samples=1064, best=7.791750e-02)\n",
      "Optimizer(iteration=134, samples=1072, best=7.528077e-02)\n",
      "Optimizer(iteration=135, samples=1080, best=7.528077e-02)\n",
      "Optimizer(iteration=136, samples=1088, best=7.411162e-02)\n",
      "Optimizer(iteration=137, samples=1096, best=7.411162e-02)\n",
      "Optimizer(iteration=138, samples=1104, best=7.351212e-02)\n",
      "Optimizer(iteration=139, samples=1112, best=7.351212e-02)\n",
      "Optimizer(iteration=140, samples=1120, best=7.351212e-02)\n",
      "Optimizer(iteration=141, samples=1128, best=7.351212e-02)\n",
      "Optimizer(iteration=142, samples=1136, best=7.351212e-02)\n",
      "Optimizer(iteration=143, samples=1144, best=7.351212e-02)\n",
      "Optimizer(iteration=144, samples=1152, best=7.351212e-02)\n",
      "Optimizer(iteration=145, samples=1160, best=7.351212e-02)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer(iteration=146, samples=1168, best=7.351212e-02)\n",
      "Optimizer(iteration=147, samples=1176, best=7.351212e-02)\n",
      "Optimizer(iteration=148, samples=1184, best=7.119973e-02)\n",
      "Optimizer(iteration=149, samples=1192, best=6.943941e-02)\n",
      "Optimizer(iteration=150, samples=1200, best=6.943941e-02)\n",
      "Optimizer(iteration=151, samples=1208, best=6.943941e-02)\n",
      "Optimizer(iteration=152, samples=1216, best=6.943941e-02)\n",
      "Optimizer(iteration=153, samples=1224, best=6.943941e-02)\n",
      "Optimizer(iteration=154, samples=1232, best=6.943941e-02)\n",
      "Optimizer(iteration=155, samples=1240, best=6.943941e-02)\n",
      "Optimizer(iteration=156, samples=1248, best=6.943941e-02)\n",
      "Optimizer(iteration=157, samples=1256, best=6.943941e-02)\n",
      "Optimizer(iteration=158, samples=1264, best=6.943941e-02)\n",
      "Optimizer(iteration=159, samples=1272, best=6.943941e-02)\n",
      "Optimizer(iteration=160, samples=1280, best=6.943941e-02)\n",
      "Optimizer(iteration=161, samples=1288, best=6.943941e-02)\n",
      "Optimizer(iteration=162, samples=1296, best=6.943941e-02)\n",
      "Optimizer(iteration=163, samples=1304, best=6.943941e-02)\n",
      "Optimizer(iteration=164, samples=1312, best=6.943941e-02)\n",
      "Optimizer(iteration=165, samples=1320, best=6.943941e-02)\n",
      "Optimizer(iteration=166, samples=1328, best=6.777830e-02)\n",
      "Optimizer(iteration=167, samples=1336, best=6.777830e-02)\n",
      "Optimizer(iteration=168, samples=1344, best=6.777830e-02)\n",
      "Optimizer(iteration=169, samples=1352, best=6.777830e-02)\n",
      "Optimizer(iteration=170, samples=1360, best=6.777830e-02)\n",
      "Optimizer(iteration=171, samples=1368, best=6.777830e-02)\n",
      "Optimizer(iteration=172, samples=1376, best=6.461948e-02)\n",
      "Optimizer(iteration=173, samples=1384, best=6.461948e-02)\n",
      "Optimizer(iteration=174, samples=1392, best=6.461948e-02)\n",
      "Optimizer(iteration=175, samples=1400, best=6.461948e-02)\n",
      "Optimizer(iteration=176, samples=1408, best=6.461948e-02)\n",
      "Optimizer(iteration=177, samples=1416, best=6.461948e-02)\n",
      "Optimizer(iteration=178, samples=1424, best=6.461948e-02)\n",
      "Optimizer(iteration=179, samples=1432, best=6.461948e-02)\n",
      "Optimizer(iteration=180, samples=1440, best=6.461948e-02)\n",
      "Optimizer(iteration=181, samples=1448, best=6.461948e-02)\n",
      "Optimizer(iteration=182, samples=1456, best=6.461948e-02)\n",
      "Optimizer(iteration=183, samples=1464, best=6.436463e-02)\n",
      "Optimizer(iteration=184, samples=1472, best=6.436463e-02)\n",
      "Optimizer(iteration=185, samples=1480, best=6.436463e-02)\n",
      "Optimizer(iteration=186, samples=1488, best=6.436463e-02)\n",
      "Optimizer(iteration=187, samples=1496, best=6.436463e-02)\n",
      "Optimizer(iteration=188, samples=1504, best=6.351529e-02)\n",
      "Optimizer(iteration=189, samples=1512, best=6.351529e-02)\n",
      "Optimizer(iteration=190, samples=1520, best=6.351529e-02)\n",
      "Optimizer(iteration=191, samples=1528, best=6.351529e-02)\n",
      "Optimizer(iteration=192, samples=1536, best=6.325208e-02)\n",
      "Optimizer(iteration=193, samples=1544, best=6.325208e-02)\n",
      "Optimizer(iteration=194, samples=1552, best=6.325208e-02)\n",
      "Optimizer(iteration=195, samples=1560, best=6.325208e-02)\n",
      "Optimizer(iteration=196, samples=1568, best=6.301195e-02)\n",
      "Optimizer(iteration=197, samples=1576, best=6.245033e-02)\n",
      "Optimizer(iteration=198, samples=1584, best=6.245033e-02)\n",
      "Optimizer(iteration=199, samples=1592, best=6.245033e-02)\n",
      "Optimizer(iteration=200, samples=1600, best=6.245033e-02)\n",
      "Optimizer(iteration=201, samples=1608, best=6.245033e-02)\n",
      "Optimizer(iteration=202, samples=1616, best=6.245033e-02)\n",
      "Optimizer(iteration=203, samples=1624, best=6.132408e-02)\n",
      "Optimizer(iteration=204, samples=1632, best=6.117485e-02)\n",
      "Optimizer(iteration=205, samples=1640, best=6.060854e-02)\n",
      "Optimizer(iteration=206, samples=1648, best=6.041738e-02)\n",
      "Optimizer(iteration=207, samples=1656, best=6.041738e-02)\n",
      "Optimizer(iteration=208, samples=1664, best=6.041738e-02)\n",
      "Optimizer(iteration=209, samples=1672, best=6.041738e-02)\n",
      "Optimizer(iteration=210, samples=1680, best=6.034959e-02)\n",
      "Optimizer(iteration=211, samples=1688, best=6.034959e-02)\n",
      "Optimizer(iteration=212, samples=1696, best=6.034959e-02)\n",
      "Optimizer(iteration=213, samples=1704, best=5.985439e-02)\n",
      "Optimizer(iteration=214, samples=1712, best=5.985439e-02)\n",
      "Optimizer(iteration=215, samples=1720, best=5.935117e-02)\n",
      "Optimizer(iteration=216, samples=1728, best=5.935117e-02)\n",
      "Optimizer(iteration=217, samples=1736, best=5.935117e-02)\n",
      "Optimizer(iteration=218, samples=1744, best=5.868096e-02)\n",
      "Optimizer(iteration=219, samples=1752, best=5.868096e-02)\n",
      "Optimizer(iteration=220, samples=1760, best=5.868096e-02)\n",
      "Optimizer(iteration=221, samples=1768, best=5.868096e-02)\n",
      "Optimizer(iteration=222, samples=1776, best=5.868096e-02)\n",
      "Optimizer(iteration=223, samples=1784, best=5.785459e-02)\n",
      "Optimizer(iteration=224, samples=1792, best=5.785459e-02)\n",
      "Optimizer(iteration=225, samples=1800, best=5.785459e-02)\n",
      "Optimizer(iteration=226, samples=1808, best=5.785459e-02)\n",
      "Optimizer(iteration=227, samples=1816, best=5.785459e-02)\n",
      "Optimizer(iteration=228, samples=1824, best=5.785459e-02)\n",
      "Optimizer(iteration=229, samples=1832, best=5.785459e-02)\n",
      "Optimizer(iteration=230, samples=1840, best=5.775970e-02)\n",
      "Optimizer(iteration=231, samples=1848, best=5.775970e-02)\n",
      "Optimizer(iteration=232, samples=1856, best=5.775970e-02)\n",
      "Optimizer(iteration=233, samples=1864, best=5.775970e-02)\n",
      "Optimizer(iteration=234, samples=1872, best=5.775970e-02)\n",
      "Optimizer(iteration=235, samples=1880, best=5.775970e-02)\n",
      "Optimizer(iteration=236, samples=1888, best=5.735885e-02)\n",
      "Optimizer(iteration=237, samples=1896, best=5.652452e-02)\n",
      "Optimizer(iteration=238, samples=1904, best=5.652452e-02)\n",
      "Optimizer(iteration=239, samples=1912, best=5.652452e-02)\n",
      "Optimizer(iteration=240, samples=1920, best=5.652452e-02)\n",
      "Optimizer(iteration=241, samples=1928, best=5.652452e-02)\n",
      "Optimizer(iteration=242, samples=1936, best=5.652452e-02)\n",
      "Optimizer(iteration=243, samples=1944, best=5.652452e-02)\n",
      "Optimizer(iteration=244, samples=1952, best=5.652452e-02)\n",
      "Optimizer(iteration=245, samples=1960, best=5.628106e-02)\n",
      "Optimizer(iteration=246, samples=1968, best=5.628106e-02)\n",
      "Optimizer(iteration=247, samples=1976, best=5.628106e-02)\n",
      "Optimizer(iteration=248, samples=1984, best=5.613132e-02)\n",
      "Optimizer(iteration=249, samples=1992, best=5.613132e-02)\n",
      "Optimizer(iteration=250, samples=2000, best=5.601767e-02)\n",
      "Optimizer(iteration=251, samples=2008, best=5.601767e-02)\n",
      "Optimizer(iteration=252, samples=2016, best=5.601767e-02)\n",
      "Optimizer(iteration=253, samples=2024, best=5.590224e-02)\n",
      "Optimizer(iteration=254, samples=2032, best=5.590224e-02)\n",
      "Optimizer(iteration=255, samples=2040, best=5.590224e-02)\n",
      "Optimizer(iteration=256, samples=2048, best=5.574728e-02)\n",
      "Optimizer(iteration=257, samples=2056, best=5.574728e-02)\n",
      "Optimizer(iteration=258, samples=2064, best=5.527350e-02)\n",
      "Optimizer(iteration=259, samples=2072, best=5.527350e-02)\n",
      "Optimizer(iteration=260, samples=2080, best=5.354580e-02)\n",
      "Optimizer(iteration=261, samples=2088, best=5.354580e-02)\n",
      "Optimizer(iteration=262, samples=2096, best=5.354580e-02)\n",
      "Optimizer(iteration=263, samples=2104, best=5.354580e-02)\n",
      "Optimizer(iteration=264, samples=2112, best=5.354580e-02)\n",
      "Optimizer(iteration=265, samples=2120, best=5.354580e-02)\n",
      "Optimizer(iteration=266, samples=2128, best=5.354580e-02)\n",
      "Optimizer(iteration=267, samples=2136, best=5.354580e-02)\n",
      "Optimizer(iteration=268, samples=2144, best=5.305965e-02)\n",
      "Optimizer(iteration=269, samples=2152, best=5.305965e-02)\n",
      "Optimizer(iteration=270, samples=2160, best=5.305965e-02)\n",
      "Optimizer(iteration=271, samples=2168, best=5.305965e-02)\n",
      "Optimizer(iteration=272, samples=2176, best=5.305965e-02)\n",
      "Optimizer(iteration=273, samples=2184, best=5.305965e-02)\n",
      "Optimizer(iteration=274, samples=2192, best=5.305965e-02)\n",
      "Optimizer(iteration=275, samples=2200, best=5.305965e-02)\n",
      "Optimizer(iteration=276, samples=2208, best=5.305965e-02)\n",
      "Optimizer(iteration=277, samples=2216, best=5.305965e-02)\n",
      "Optimizer(iteration=278, samples=2224, best=5.305965e-02)\n",
      "Optimizer(iteration=279, samples=2232, best=5.305965e-02)\n",
      "Optimizer(iteration=280, samples=2240, best=5.290415e-02)\n",
      "Optimizer(iteration=281, samples=2248, best=5.290415e-02)\n",
      "Optimizer(iteration=282, samples=2256, best=5.290415e-02)\n",
      "Optimizer(iteration=283, samples=2264, best=5.290415e-02)\n",
      "Optimizer(iteration=284, samples=2272, best=5.284740e-02)\n",
      "Optimizer(iteration=285, samples=2280, best=5.284740e-02)\n",
      "Optimizer(iteration=286, samples=2288, best=5.282644e-02)\n",
      "Optimizer(iteration=287, samples=2296, best=5.200036e-02)\n",
      "Optimizer(iteration=288, samples=2304, best=5.188871e-02)\n",
      "Optimizer(iteration=289, samples=2312, best=5.188871e-02)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer(iteration=290, samples=2320, best=5.188871e-02)\n",
      "Optimizer(iteration=291, samples=2328, best=5.188871e-02)\n",
      "Optimizer(iteration=292, samples=2336, best=5.188871e-02)\n",
      "Optimizer(iteration=293, samples=2344, best=5.188871e-02)\n",
      "Optimizer(iteration=294, samples=2352, best=5.188871e-02)\n",
      "Optimizer(iteration=295, samples=2360, best=5.188871e-02)\n",
      "Optimizer(iteration=296, samples=2368, best=5.188871e-02)\n",
      "Optimizer(iteration=297, samples=2376, best=5.188871e-02)\n",
      "Optimizer(iteration=298, samples=2384, best=5.188871e-02)\n",
      "Optimizer(iteration=299, samples=2392, best=5.188871e-02)\n",
      "Optimizer(iteration=300, samples=2400, best=5.154017e-02)\n",
      "Optimizer(iteration=301, samples=2408, best=5.154017e-02)\n",
      "Optimizer(iteration=302, samples=2416, best=5.134153e-02)\n",
      "Optimizer(iteration=303, samples=2424, best=5.085940e-02)\n",
      "Optimizer(iteration=304, samples=2432, best=5.085940e-02)\n",
      "Optimizer(iteration=305, samples=2440, best=5.085940e-02)\n",
      "Optimizer(iteration=306, samples=2448, best=5.085940e-02)\n",
      "Optimizer(iteration=307, samples=2456, best=5.085940e-02)\n",
      "Optimizer(iteration=308, samples=2464, best=5.085940e-02)\n",
      "Optimizer(iteration=309, samples=2472, best=5.081711e-02)\n",
      "Optimizer(iteration=310, samples=2480, best=5.081711e-02)\n",
      "Optimizer(iteration=311, samples=2488, best=5.081711e-02)\n",
      "Optimizer(iteration=312, samples=2496, best=5.081711e-02)\n",
      "Optimizer(iteration=313, samples=2504, best=5.081711e-02)\n",
      "Optimizer(iteration=314, samples=2512, best=5.072664e-02)\n",
      "Optimizer(iteration=315, samples=2520, best=5.004481e-02)\n",
      "Optimizer(iteration=316, samples=2528, best=5.004481e-02)\n",
      "Optimizer(iteration=317, samples=2536, best=5.004481e-02)\n",
      "Optimizer(iteration=318, samples=2544, best=4.936620e-02)\n",
      "Optimizer(iteration=319, samples=2552, best=4.936620e-02)\n",
      "Optimizer(iteration=320, samples=2560, best=4.936620e-02)\n",
      "Optimizer(iteration=321, samples=2568, best=4.936620e-02)\n",
      "Optimizer(iteration=322, samples=2576, best=4.936620e-02)\n",
      "Optimizer(iteration=323, samples=2584, best=4.936620e-02)\n",
      "Optimizer(iteration=324, samples=2592, best=4.929589e-02)\n",
      "Optimizer(iteration=325, samples=2600, best=4.929589e-02)\n",
      "Optimizer(iteration=326, samples=2608, best=4.929589e-02)\n",
      "Optimizer(iteration=327, samples=2616, best=4.929589e-02)\n",
      "Optimizer(iteration=328, samples=2624, best=4.929216e-02)\n",
      "Optimizer(iteration=329, samples=2632, best=4.929216e-02)\n",
      "Optimizer(iteration=330, samples=2640, best=4.929216e-02)\n",
      "Optimizer(iteration=331, samples=2648, best=4.929216e-02)\n",
      "Optimizer(iteration=332, samples=2656, best=4.923414e-02)\n",
      "Optimizer(iteration=333, samples=2664, best=4.883334e-02)\n",
      "Optimizer(iteration=334, samples=2672, best=4.883334e-02)\n",
      "Optimizer(iteration=335, samples=2680, best=4.883334e-02)\n",
      "Optimizer(iteration=336, samples=2688, best=4.871799e-02)\n",
      "Optimizer(iteration=337, samples=2696, best=4.871799e-02)\n",
      "Optimizer(iteration=338, samples=2704, best=4.835681e-02)\n",
      "Optimizer(iteration=339, samples=2712, best=4.835681e-02)\n",
      "Optimizer(iteration=340, samples=2720, best=4.835681e-02)\n",
      "Optimizer(iteration=341, samples=2728, best=4.835681e-02)\n",
      "Optimizer(iteration=342, samples=2736, best=4.835681e-02)\n",
      "Optimizer(iteration=343, samples=2744, best=4.835681e-02)\n",
      "Optimizer(iteration=344, samples=2752, best=4.835681e-02)\n",
      "Optimizer(iteration=345, samples=2760, best=4.819572e-02)\n",
      "Optimizer(iteration=346, samples=2768, best=4.794400e-02)\n",
      "Optimizer(iteration=347, samples=2776, best=4.794400e-02)\n",
      "Optimizer(iteration=348, samples=2784, best=4.775177e-02)\n",
      "Optimizer(iteration=349, samples=2792, best=4.690271e-02)\n",
      "Optimizer(iteration=350, samples=2800, best=4.690271e-02)\n",
      "Optimizer(iteration=351, samples=2808, best=4.690271e-02)\n",
      "Optimizer(iteration=352, samples=2816, best=4.690271e-02)\n",
      "Optimizer(iteration=353, samples=2824, best=4.690271e-02)\n",
      "Optimizer(iteration=354, samples=2832, best=4.690271e-02)\n",
      "Optimizer(iteration=355, samples=2840, best=4.690271e-02)\n",
      "Optimizer(iteration=356, samples=2848, best=4.690271e-02)\n",
      "Optimizer(iteration=357, samples=2856, best=4.690271e-02)\n",
      "Optimizer(iteration=358, samples=2864, best=4.690271e-02)\n",
      "Optimizer(iteration=359, samples=2872, best=4.690271e-02)\n",
      "Optimizer(iteration=360, samples=2880, best=4.690271e-02)\n",
      "Optimizer(iteration=361, samples=2888, best=4.690271e-02)\n",
      "Optimizer(iteration=362, samples=2896, best=4.690271e-02)\n",
      "Optimizer(iteration=363, samples=2904, best=4.690271e-02)\n",
      "Optimizer(iteration=364, samples=2912, best=4.690271e-02)\n",
      "Optimizer(iteration=365, samples=2920, best=4.690271e-02)\n",
      "Optimizer(iteration=366, samples=2928, best=4.634373e-02)\n",
      "Optimizer(iteration=367, samples=2936, best=4.606528e-02)\n",
      "Optimizer(iteration=368, samples=2944, best=4.606528e-02)\n",
      "Optimizer(iteration=369, samples=2952, best=4.606528e-02)\n",
      "Optimizer(iteration=370, samples=2960, best=4.606528e-02)\n",
      "Optimizer(iteration=371, samples=2968, best=4.606528e-02)\n",
      "Optimizer(iteration=372, samples=2976, best=4.606528e-02)\n",
      "Optimizer(iteration=373, samples=2984, best=4.532729e-02)\n",
      "Optimizer(iteration=374, samples=2992, best=4.532729e-02)\n",
      "Optimizer(iteration=375, samples=3000, best=4.532729e-02)\n",
      "Optimizer(iteration=376, samples=3008, best=4.532729e-02)\n",
      "Optimizer(iteration=377, samples=3016, best=4.532729e-02)\n",
      "Optimizer(iteration=378, samples=3024, best=4.532729e-02)\n",
      "Optimizer(iteration=379, samples=3032, best=4.532729e-02)\n",
      "Optimizer(iteration=380, samples=3040, best=4.301760e-02)\n",
      "Optimizer(iteration=381, samples=3048, best=4.301760e-02)\n",
      "Optimizer(iteration=382, samples=3056, best=4.301760e-02)\n",
      "Optimizer(iteration=383, samples=3064, best=4.301760e-02)\n",
      "Optimizer(iteration=384, samples=3072, best=4.301760e-02)\n",
      "Optimizer(iteration=385, samples=3080, best=4.301760e-02)\n",
      "Optimizer(iteration=386, samples=3088, best=4.301760e-02)\n",
      "Optimizer(iteration=387, samples=3096, best=4.301760e-02)\n",
      "Optimizer(iteration=388, samples=3104, best=4.301760e-02)\n",
      "Optimizer(iteration=389, samples=3112, best=4.285446e-02)\n",
      "Optimizer(iteration=390, samples=3120, best=4.203135e-02)\n",
      "Optimizer(iteration=391, samples=3128, best=4.203135e-02)\n",
      "Optimizer(iteration=392, samples=3136, best=4.185085e-02)\n",
      "Optimizer(iteration=393, samples=3144, best=4.185085e-02)\n",
      "Optimizer(iteration=394, samples=3152, best=4.185085e-02)\n",
      "Optimizer(iteration=395, samples=3160, best=4.185085e-02)\n",
      "Optimizer(iteration=396, samples=3168, best=4.185085e-02)\n",
      "Optimizer(iteration=397, samples=3176, best=4.185085e-02)\n",
      "Optimizer(iteration=398, samples=3184, best=4.185085e-02)\n",
      "Optimizer(iteration=399, samples=3192, best=4.185085e-02)\n",
      "Optimizer(iteration=400, samples=3200, best=4.185085e-02)\n",
      "Optimizer(iteration=401, samples=3208, best=4.185085e-02)\n",
      "Optimizer(iteration=402, samples=3216, best=4.185085e-02)\n",
      "Optimizer(iteration=403, samples=3224, best=4.185085e-02)\n",
      "Optimizer(iteration=404, samples=3232, best=4.185085e-02)\n",
      "Optimizer(iteration=405, samples=3240, best=4.132880e-02)\n",
      "Optimizer(iteration=406, samples=3248, best=4.132880e-02)\n",
      "Optimizer(iteration=407, samples=3256, best=4.046957e-02)\n",
      "Optimizer(iteration=408, samples=3264, best=4.046957e-02)\n",
      "Optimizer(iteration=409, samples=3272, best=4.046957e-02)\n",
      "Optimizer(iteration=410, samples=3280, best=4.046957e-02)\n",
      "Optimizer(iteration=411, samples=3288, best=4.046957e-02)\n",
      "Optimizer(iteration=412, samples=3296, best=4.046957e-02)\n",
      "Optimizer(iteration=413, samples=3304, best=4.024572e-02)\n",
      "Optimizer(iteration=414, samples=3312, best=4.024572e-02)\n",
      "Optimizer(iteration=415, samples=3320, best=4.024572e-02)\n",
      "Optimizer(iteration=416, samples=3328, best=4.017342e-02)\n",
      "Optimizer(iteration=417, samples=3336, best=3.984193e-02)\n",
      "Optimizer(iteration=418, samples=3344, best=3.937831e-02)\n",
      "Optimizer(iteration=419, samples=3352, best=3.910938e-02)\n",
      "Optimizer(iteration=420, samples=3360, best=3.910938e-02)\n",
      "Optimizer(iteration=421, samples=3368, best=3.910938e-02)\n",
      "Optimizer(iteration=422, samples=3376, best=3.826436e-02)\n",
      "Optimizer(iteration=423, samples=3384, best=3.826436e-02)\n",
      "Optimizer(iteration=424, samples=3392, best=3.826436e-02)\n",
      "Optimizer(iteration=425, samples=3400, best=3.826436e-02)\n",
      "Optimizer(iteration=426, samples=3408, best=3.826436e-02)\n",
      "Optimizer(iteration=427, samples=3416, best=3.826436e-02)\n",
      "Optimizer(iteration=428, samples=3424, best=3.826436e-02)\n",
      "Optimizer(iteration=429, samples=3432, best=3.826436e-02)\n",
      "Optimizer(iteration=430, samples=3440, best=3.826436e-02)\n",
      "Optimizer(iteration=431, samples=3448, best=3.826436e-02)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer(iteration=432, samples=3456, best=3.826436e-02)\n",
      "Optimizer(iteration=433, samples=3464, best=3.826436e-02)\n",
      "Optimizer(iteration=434, samples=3472, best=3.826436e-02)\n",
      "Optimizer(iteration=435, samples=3480, best=3.826436e-02)\n",
      "Optimizer(iteration=436, samples=3488, best=3.826436e-02)\n",
      "Optimizer(iteration=437, samples=3496, best=3.799109e-02)\n",
      "Optimizer(iteration=438, samples=3504, best=3.799109e-02)\n",
      "Optimizer(iteration=439, samples=3512, best=3.799109e-02)\n",
      "Optimizer(iteration=440, samples=3520, best=3.752369e-02)\n",
      "Optimizer(iteration=441, samples=3528, best=3.752369e-02)\n",
      "Optimizer(iteration=442, samples=3536, best=3.752369e-02)\n",
      "Optimizer(iteration=443, samples=3544, best=3.752369e-02)\n",
      "Optimizer(iteration=444, samples=3552, best=3.752369e-02)\n",
      "Optimizer(iteration=445, samples=3560, best=3.752369e-02)\n",
      "Optimizer(iteration=446, samples=3568, best=3.752369e-02)\n",
      "Optimizer(iteration=447, samples=3576, best=3.752369e-02)\n",
      "Optimizer(iteration=448, samples=3584, best=3.752369e-02)\n",
      "Optimizer(iteration=449, samples=3592, best=3.737949e-02)\n",
      "Optimizer(iteration=450, samples=3600, best=3.737949e-02)\n",
      "Optimizer(iteration=451, samples=3608, best=3.737949e-02)\n",
      "Optimizer(iteration=452, samples=3616, best=3.737949e-02)\n",
      "Optimizer(iteration=453, samples=3624, best=3.719704e-02)\n",
      "Optimizer(iteration=454, samples=3632, best=3.719704e-02)\n",
      "Optimizer(iteration=455, samples=3640, best=3.719704e-02)\n",
      "Optimizer(iteration=456, samples=3648, best=3.703329e-02)\n",
      "Optimizer(iteration=457, samples=3656, best=3.703329e-02)\n",
      "Optimizer(iteration=458, samples=3664, best=3.634734e-02)\n",
      "Optimizer(iteration=459, samples=3672, best=3.634734e-02)\n",
      "Optimizer(iteration=460, samples=3680, best=3.634734e-02)\n",
      "Optimizer(iteration=461, samples=3688, best=3.634734e-02)\n",
      "Optimizer(iteration=462, samples=3696, best=3.615457e-02)\n",
      "Optimizer(iteration=463, samples=3704, best=3.615457e-02)\n",
      "Optimizer(iteration=464, samples=3712, best=3.615457e-02)\n",
      "Optimizer(iteration=465, samples=3720, best=3.615457e-02)\n",
      "Optimizer(iteration=466, samples=3728, best=3.594164e-02)\n",
      "Optimizer(iteration=467, samples=3736, best=3.594164e-02)\n",
      "Optimizer(iteration=468, samples=3744, best=3.594164e-02)\n",
      "Optimizer(iteration=469, samples=3752, best=3.594164e-02)\n",
      "Optimizer(iteration=470, samples=3760, best=3.588228e-02)\n",
      "Optimizer(iteration=471, samples=3768, best=3.588228e-02)\n",
      "Optimizer(iteration=472, samples=3776, best=3.588228e-02)\n",
      "Optimizer(iteration=473, samples=3784, best=3.533377e-02)\n",
      "Optimizer(iteration=474, samples=3792, best=3.533377e-02)\n",
      "Optimizer(iteration=475, samples=3800, best=3.533377e-02)\n",
      "Optimizer(iteration=476, samples=3808, best=3.533377e-02)\n",
      "Optimizer(iteration=477, samples=3816, best=3.533377e-02)\n",
      "Optimizer(iteration=478, samples=3824, best=3.533377e-02)\n",
      "Optimizer(iteration=479, samples=3832, best=3.533377e-02)\n",
      "Optimizer(iteration=480, samples=3840, best=3.533377e-02)\n",
      "Optimizer(iteration=481, samples=3848, best=3.533377e-02)\n",
      "Optimizer(iteration=482, samples=3856, best=3.533377e-02)\n",
      "Optimizer(iteration=483, samples=3864, best=3.450155e-02)\n",
      "Optimizer(iteration=484, samples=3872, best=3.450155e-02)\n",
      "Optimizer(iteration=485, samples=3880, best=3.450155e-02)\n",
      "Optimizer(iteration=486, samples=3888, best=3.442603e-02)\n",
      "Optimizer(iteration=487, samples=3896, best=3.442603e-02)\n",
      "Optimizer(iteration=488, samples=3904, best=3.442603e-02)\n",
      "Optimizer(iteration=489, samples=3912, best=3.366822e-02)\n",
      "Optimizer(iteration=490, samples=3920, best=3.366822e-02)\n",
      "Optimizer(iteration=491, samples=3928, best=3.366822e-02)\n",
      "Optimizer(iteration=492, samples=3936, best=3.366822e-02)\n",
      "Optimizer(iteration=493, samples=3944, best=3.366822e-02)\n",
      "Optimizer(iteration=494, samples=3952, best=3.366822e-02)\n",
      "Optimizer(iteration=495, samples=3960, best=3.331276e-02)\n",
      "Optimizer(iteration=496, samples=3968, best=3.331276e-02)\n",
      "Optimizer(iteration=497, samples=3976, best=3.331276e-02)\n",
      "Optimizer(iteration=498, samples=3984, best=3.331276e-02)\n",
      "Optimizer(iteration=499, samples=3992, best=3.331276e-02)\n",
      "Optimizer(iteration=500, samples=4000, best=3.331276e-02)\n",
      "Optimizer(iteration=501, samples=4008, best=3.331276e-02)\n",
      "Optimizer(iteration=502, samples=4016, best=3.331276e-02)\n",
      "Optimizer(iteration=503, samples=4024, best=3.331276e-02)\n",
      "Optimizer(iteration=504, samples=4032, best=3.331276e-02)\n",
      "Optimizer(iteration=505, samples=4040, best=3.331276e-02)\n",
      "Optimizer(iteration=506, samples=4048, best=3.331276e-02)\n",
      "Optimizer(iteration=507, samples=4056, best=3.276902e-02)\n",
      "Optimizer(iteration=508, samples=4064, best=3.225721e-02)\n",
      "Optimizer(iteration=509, samples=4072, best=3.225721e-02)\n",
      "Optimizer(iteration=510, samples=4080, best=3.225721e-02)\n",
      "Optimizer(iteration=511, samples=4088, best=3.225721e-02)\n",
      "Optimizer(iteration=512, samples=4096, best=3.156516e-02)\n",
      "Optimizer(iteration=513, samples=4104, best=3.105216e-02)\n",
      "Optimizer(iteration=514, samples=4112, best=3.105216e-02)\n",
      "Optimizer(iteration=515, samples=4120, best=3.105216e-02)\n",
      "Optimizer(iteration=516, samples=4128, best=3.105216e-02)\n",
      "Optimizer(iteration=517, samples=4136, best=3.105216e-02)\n",
      "Optimizer(iteration=518, samples=4144, best=3.105216e-02)\n",
      "Optimizer(iteration=519, samples=4152, best=3.105216e-02)\n",
      "Optimizer(iteration=520, samples=4160, best=3.105216e-02)\n",
      "Optimizer(iteration=521, samples=4168, best=3.105216e-02)\n",
      "Optimizer(iteration=522, samples=4176, best=3.105216e-02)\n",
      "Optimizer(iteration=523, samples=4184, best=3.103939e-02)\n",
      "Optimizer(iteration=524, samples=4192, best=3.057749e-02)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\2019881752.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mnumIters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmisfit\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdidNotconverge\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# While misfit is below the threshold and we haven't failed to converge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0msrch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mmisfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mnumIters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumIters\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\2683724276.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, num_iter)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_objective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                 self._sample.append({\n\u001b[0;32m     63\u001b[0m                     \u001b[1;34m'param'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\3051610226.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\4094136712.py\u001b[0m in \u001b[0;36mobjectiveFunction\u001b[1;34m(X, data)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mfwdModelResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforwardModelKinetics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Parameters that need to be read in (These I'll likely read from a file eventually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\439550392.py\u001b[0m in \u001b[0;36mforwardModelKinetics\u001b[1;34m(kinetics, expData)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m#Calculate D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mDaaForSum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDaa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtsec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mDaaForSum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDaa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcumtsec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mcumtsec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mDtaa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDaaForSum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# I wrote this gross while loop to try to test out the possible values of the model\n",
    "\n",
    "\n",
    "threshold = .01\n",
    "num_dim = 6\n",
    "num_sampToTry = range(3,21)\n",
    "num_ResampToTry = range(3,20)\n",
    "misfitVals = np.ones([101,101])*10*11\n",
    "numIterations4Save = np.ones([100,101])*10*11\n",
    "durations = np.ones([101,101])*10*11\n",
    "\n",
    "\n",
    "\n",
    "for i in num_sampToTry:\n",
    "    j=i\n",
    "    while j <=100:\n",
    "\n",
    "        misfit = 10**11\n",
    "        counter = 0\n",
    "        print(\"num_samp = \" +str(j))\n",
    "        print(\"num_resamp = \" +str(i))\n",
    "        srch = Optimizer(\n",
    "                objective_fn=objectiveFunction,\n",
    "                names = [\"Ea\",\"LnD0aa1\",\"LnD0aa2\",\"LnD0aa3\",\"Frac1\",\"Frac2\"],\n",
    "                limits=[(70,110),(0,25),(0,25),(0,25),(10**(-10),1),(10**(-10),1)], \n",
    "                dataset_path = nameOfExperimentalResultsFile,\n",
    "                num_samp=8, #number of random samples taken at each iteration\n",
    "                num_resamp=4, #number of best Voronoi polygons sampled at each iteration-- must be smaller than num_samp\n",
    "                maximize=False,\n",
    "                verbose=True\n",
    "                )\n",
    "        start_time = time.time()\n",
    "        didNotconverge = 0\n",
    "        numIters = 0\n",
    "        while (misfit >= threshold) and (didNotconverge == 0): # While misfit is below the threshold and we haven't failed to converge\n",
    "            srch.update(100)\n",
    "            misfit = srch.sample[0][\"result\"]\n",
    "            numIters = numIters+1\n",
    "            if numIters > 2000:\n",
    "                didNotconverge = 1\n",
    "            elif (numIters > 100) and (srch.sample[0][\"result\"] == srch.sample[75][\"result\"]):\n",
    "                didNotConverge = 1\n",
    "        if didNotconverge != 1: #This means we converged\n",
    "            misfitVals[i,j] = misfit\n",
    "            durations[i,j] = (time.time() - start_time)\n",
    "            numIterations4Save[i,j] = numIters\n",
    "            j=j+1\n",
    "  \n",
    "        \n",
    "        \n",
    "#     # Extract the best fitting model with this while loop\n",
    "#         exitFlag = 0\n",
    "#         counter = 0\n",
    "#         while exitFlag == 0:\n",
    "#             if srch.sample[counter]['iter'] == n_iter-1:\n",
    "#                 bestFit = srch.sample[counter]\n",
    "#                 exitFlag = 1\n",
    "#             else:\n",
    "#                 counter += 1\n",
    "\n",
    "#         misfitVals[i-1,j-1] = srch.sample[counter][\"result\"]\n",
    "\n",
    "# fwdModel = forwardModelKinetics(srch.sample[counter]['param'],diffusionExperimentResults)\n",
    "# plotModelResults(fwdModel,diffusionExperimentResults)\n",
    "# bestFit\n",
    "\n",
    "\n",
    "np.savetxt(\"misfitVals.csv\", misfitVals,delimiter = ',')\n",
    "np.savetxt(\"numiters.csv\", numIterations4Save, delimiter = ',')\n",
    "np.savetxt(\"durations.csv\", durations, delimiter = ',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
